{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of the paper you mentioned, the parameter $q_i$ represents the multiplicity of the $i$-th column (feature or datapoint) in the current dictionary $I_t$, which is updated at each iteration of the algorithm.\n",
    "\n",
    "The role of the parameter $q_i$ is to keep track of how many times the $i$-th column has been selected in previous iterations of the algorithm. This is relevant because the algorithm uses a stochastic sampling strategy, where columns are selected with probability proportional to their ridge leverage score (RLS), which is a measure of their importance for approximating the output.\n",
    "\n",
    "If a column has a high RLS and has never been selected in previous iterations, it is likely to be selected in the current iteration. However, if the column has a low RLS but has been selected many times in previous iterations, it may be skipped in the current iteration in favor of other columns with higher RLS.\n",
    "\n",
    "The parameter $q_i$ is used to adjust the probabilities of sampling a column based on its multiplicity in the dictionary. Specifically, the probability of choosing the $i$-th column in the current iteration is proportional to $\\widetilde{p}_i q_i$, where $\\widetilde{p}_i$ is the probability computed using Equation (1) in the paper and $q_i$ is the multiplicity of the $i$-th column in the current dictionary. This ensures that columns that have been selected many times in previous iterations are less likely to be selected again, while still allowing for some exploration of the feature space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code part\n",
    "$$\\widetilde{\\tau}_{t,i}=\\frac{1-\\epsilon}{\\gamma}\\left( k_{i,i}-\\boldsymbol{k}_{t,i}^\\top \\overline{\\boldsymbol{S}}(\\overline{\\boldsymbol{S}}\\boldsymbol{K}_t\\overline{\\boldsymbol{S}}+\\gamma\\boldsymbol{I}_t)^{-1}\\overline{\\boldsymbol{S}}^\\top\\boldsymbol{k}_{t,i} \\right)\\tag1$$\n",
    "The code implementation of compute_tau function is an efficient method to compute the ridge leverage scores based on the estimator proposed in the Calandriello et al. 2017 paper.Copy\n",
    "\n",
    "### Sherman-Morrison-Woodbury formula\n",
    "Given a square invertible $n\\times n$ matrix $A$, an $n \\times k$ matrix $U$, and a $k\\times n$ matrix $V$, let $B$ be an $n\\times n$ matrix such that $B=A+UV$. Then, assuming $$\\left(I_{k}+VA^{-1}U\\right)$$ is invertible, we have\n",
    "$$ B^{-1}=A^{-1}-A^{-1}U\\left(I_{k}+VA^{-1}U\\right)^{-1}VA^{-1}.$$\n",
    "\n",
    "The main idea of the estimator is to use the Sherman-Morrison-Woodbury formula to compute the inverse of the matrix $(\\overline{\\boldsymbol{S}}\\boldsymbol{K}_t\\overline{\\boldsymbol{S}}+\\gamma\\boldsymbol{I}_t)$ in Equation (1) without explicitly computing it. This reduces the computational cost of computing the RLS, especially when the matrix size is large.\n",
    "\n",
    "In the code implementation, the matrix $(\\overline{\\boldsymbol{S}}\\boldsymbol{K}_t\\overline{\\boldsymbol{S}}+\\gamma\\boldsymbol{I}_t)$ is approximated as $X'X +\\lambda S^{-2}$, where $X'X$ is the kernel matrix between the samples in the current dictionary $I_t$, and $S$ is the diagonal matrix of weights.\n",
    "\n",
    "### Cholesky decomposition\n",
    "\n",
    "The Cholesky decomposition of a Hermitian positive-definite matrix A, is a decomposition of the form,\n",
    "$$\\mathbf {A} =\\mathbf {LL} ^{*}$$\n",
    "where $L$ is a lower triangular matrix with real and positive diagonal entries, and $L^*$ denotes the conjugate transpose of $L$.\n",
    "\n",
    "To compute the inverse of $(X'X +\\lambda S^{-2})$, the function performs a singular value decomposition (SVD) on the kernel matrix between the samples in the dictionary, using np.linalg.svd. The SVD is used to compute the partial Cholesky factorization of $(X'X +\\lambda S^{-2})$, which is equivalent to the term $\\overline{\\boldsymbol{S}}(\\overline{\\boldsymbol{S}}\\boldsymbol{K}_t\\overline{\\boldsymbol{S}}+\\gamma\\boldsymbol{I}_t)^{-1}\\overline{\\boldsymbol{S}}^\\top$ in Equation (1). Here, S_root_inv_DD corresponds to the square root inverse of $S$, and U_DD corresponds to the left singular vectors of the kernel matrix.\n",
    "\n",
    "The matrix multiplication $\\overline{\\boldsymbol{S}}^\\top\\boldsymbol{k}{t,i}$ in Equation (1) is approximated as $E \\boldsymbol{k}{t,i}$, where $E$ is a precomputed matrix, and $\\boldsymbol{k}_{t,i}$ is the kernel vector between the $i$-th data point in the dataset and the dictionary. The matrix $E$ is computed as $E=(X'X +\\lambda S^{-2})^{-1/2} X'$.\n",
    "\n",
    "Finally, the RLS are computed using Equation (1) with the above-mentioned approximations. The resulting RLS vector is returned by the function.\n",
    "\n",
    "Therefore, the compute_tau function provides an efficient implementation of the estimator in the paper using an SVD-based approximation of the partial Cholesky factorization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code breakdown\n",
    "\n",
    "Yes, I'd be happy to break down the compute_tau function line by line with details:\n",
    "\n",
    "```py\n",
    "def compute_tau(centers_dict: CentersDictionary,\n",
    "                X: np.ndarray,\n",
    "                similarity_func: callable,\n",
    "                lam_new: float,\n",
    "                force_cpu=False):\n",
    "```\n",
    "\n",
    "This function takes as input a dictionary of centers (representative data points) called centers_dict, a dataset X, a similarity function similarity_func, a regularization parameter lam_new, and a boolean flag force_cpu indicating whether to use CPU (if True) or GPU (if False) computation.\n",
    "\n",
    "```py\n",
    "xp = __load_gpu_module(force_cpu)\n",
    "```\n",
    "This line loads the NumPy-like module to be used for computation, either NumPy or CuPy depending on the value of force_cpu.\n",
    "\n",
    "```py\n",
    "diag_norm = np.asarray(similarity_func.diag(X))\n",
    "```\n",
    "\n",
    "This line computes the diagonal entries of the kernel matrix between X and X using the provided similarity function similarity_func. The resulting array is converted to a NumPy ndarray.\n",
    "\n",
    "```py\n",
    "K_DU = xp.asarray(similarity_func(centers_dict.X, X))\n",
    "```\n",
    "\n",
    "This line computes the kernel matrix between the centers centers_dict.X and X using the provided similarity function similarity_func. The resulting matrix is converted to the computation module's format (either NumPy or CuPy) using xp.asarray.\n",
    "\n",
    "```py\n",
    "U_DD, S_DD, _ = np.linalg.svd(xp.asnumpy(similarity_func(centers_dict.X, centers_dict.X)\n",
    "                                            + lam_new * np.diag(centers_dict.probs)))\n",
    "```\n",
    "\n",
    "This line computes the singular value decomposition (SVD) of the kernel matrix between the centers centers_dict.X using NumPy's np.linalg.svd function.\n",
    "\n",
    "First, the kernel matrix between the centers is computed using the provided similarity function similarity_func, then the regularization parameter is added as a diagonal matrix of weights. The resulting kernel matrix is converted to a NumPy ndarray using xp.asnumpy.\n",
    "\n",
    "The SVD returns the left singular vectors U_DD, the singular values S_DD, and the right singular vectors (which are not needed and hence assigned to _).\n",
    "\n",
    "```py\n",
    "U_DD, S_root_inv_DD = __stable_invert_root(U_DD, S_DD)\n",
    "```\n",
    "\n",
    "This line computes the square root inverse of the weights diagonal matrix $S^{-2}$, which is needed to perform the partial Cholesky factorization of the kernel matrix between the centers in Equation (1) of the Calandriello et al. 2017 paper. The computation is performed using the helper function __stable_invert_root.\n",
    "\n",
    "```py\n",
    "E = xp.asarray(S_root_inv_DD * U_DD.T)\n",
    "```\n",
    "\n",
    "This line computes the precomputed matrix $E=(X'X +\\lambda S^{-2})^{-1/2} X'$, which is needed to compute the RLS in Equation (1) of the Calandriello et al. 2017 paper. The computation is performed using the inverse square root of weights diagonal matrix $S^{-2}$ and the left singular vectors from the previous step.\n",
    "\n",
    "```py\n",
    "X_precond = E.dot(K_DU)\n",
    "```\n",
    "\n",
    "This line forms the matrix $(X'X +\\lambda S^{-2})^{-1/2}XX'$, which is used to compute the RLS in Equation (1) of the Calandriello et al. 2017 paper. The computation is equivalent to the term $\\overline{\\boldsymbol{S}}^\\top\\boldsymbol{k}_{t,i}$\n",
    "in Equation (1), where $\\boldsymbol{k}_{t,i}$ represents the kernel vector between the $i$-th datapoint in X and the dictionary of centers.\n",
    "\n",
    "```py\n",
    "tau = (diag_norm - xp.asnumpy(xp.square(X_precond, out=X_precond).sum(axis=0))) / lam_new\n",
    "```\n",
    "\n",
    "This line computes the ridge leverage score (RLS) using Equation (1) from the Calandriello et al. 2017 paper. The computation is performed on the precomputed matrices and vectors from the previous steps.\n",
    "\n",
    "The result is returned as a NumPy ndarray called tau, containing the RLS values for each datapoint in X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
