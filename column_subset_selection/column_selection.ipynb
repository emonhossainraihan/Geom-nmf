{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "CentersDictionary = namedtuple('CentersDictionary', ('idx', 'X', 'probs', 'lam', 'qbar'))\n",
    "\n",
    "def __load_gpu_module(force_cpu: bool):\n",
    "    xp = np\n",
    "    xp.asnumpy = np.asarray\n",
    "    if not force_cpu:\n",
    "        try:\n",
    "            import cupy as cp\n",
    "            xp = cp\n",
    "        except ImportError:\n",
    "            print(\"cupy not found, defaulting to numpy\")\n",
    "    return xp\n",
    "\n",
    "\n",
    "def __get_progress_bar(total=-1, disable=False):\n",
    "    \"\"\"Helper function to get a tqdm progress bar (or a simple fallback otherwise)\"\"\"\n",
    "    class ProgBar(object):\n",
    "        def __init__(self, total=-1, disable=False):\n",
    "            self.disable = disable\n",
    "            self.t = 0\n",
    "            self.total = total\n",
    "            self.debug_string = \"\"\n",
    "\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "\n",
    "        def __exit__(self, *args, **kwargs):\n",
    "            pass\n",
    "\n",
    "        def set_postfix(self, **kwargs):\n",
    "            self.debug_string = \"\"\n",
    "            for arg in kwargs:\n",
    "                self.debug_string += \"{}={} \".format(arg, kwargs[arg])\n",
    "\n",
    "        def update(self):\n",
    "            if not self.disable:\n",
    "                self.t += 1\n",
    "                print_str = \"{}\".format(self.t)\n",
    "\n",
    "                if self.total > 0:\n",
    "                    print_str += \"/{}\".format(self.total)\n",
    "\n",
    "                print_str += \": {}\".format(self.debug_string)\n",
    "\n",
    "                if len(print_str) < 80:\n",
    "                    print_str = print_str + \" \"*(80 - len(print_str))\n",
    "                print(print_str, end='\\r', flush=True)\n",
    "\n",
    "            if self.t == self.total:\n",
    "                print(\"\")\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "        progress_bar = tqdm(total=total, disable=disable)\n",
    "    except ImportError:\n",
    "        progress_bar = ProgBar(total=total, disable=disable)\n",
    "    return progress_bar\n",
    "\n",
    "\n",
    "def __stable_invert_root(U: np.ndarray, S: np.ndarray):\n",
    "    n = U.shape[0]\n",
    "    assert U.shape == (n, n)\n",
    "    assert S.shape == (n,)\n",
    "    # threshold formula taken from pinv2's implementation of numpy/scipy\n",
    "    thresh = S.max() * max(S.shape) * np.finfo(S.dtype).eps\n",
    "    stable_eig = np.logical_not(np.isclose(S, 0., atol=thresh))\n",
    "    m = sum(stable_eig)\n",
    "    U_thin = U[:, stable_eig]\n",
    "    S_thin = S[stable_eig]\n",
    "    assert U_thin.shape == (n, m)\n",
    "    assert S_thin.shape == (m,)\n",
    "    S_thin_inv_root = (1 / np.sqrt(S_thin)).reshape(-1, 1)\n",
    "    return U_thin, S_thin_inv_root\n",
    "\n",
    "\n",
    "def compute_tau(centers_dict: CentersDictionary,\n",
    "                X: np.ndarray,\n",
    "                similarity_func: callable,\n",
    "                lam_new: float,\n",
    "                force_cpu=False):\n",
    "    xp = __load_gpu_module(force_cpu)\n",
    "    diag_norm = np.asarray(similarity_func.diag(X))\n",
    "    # (m x n) kernel matrix between samples in dictionary and dataset X\n",
    "    K_DU = xp.asarray(similarity_func(centers_dict.X, X))\n",
    "    # the estimator proposed in Calandriello et al. 2017 is\n",
    "    # diag(XX' - XX'S(SX'XS + lam*I)^(-1)SXX')/lam\n",
    "    # here for efficiency we collect an S inside the inverse and compute\n",
    "    # diag(XX' - XX'(X'X + lam*S^(-2))^(-1)XX')/lam\n",
    "    # note that in the second term we take care of dropping the rows/columns of X associated\n",
    "    # with 0 entries in S\n",
    "    U_DD, S_DD, _ = np.linalg.svd(xp.asnumpy(similarity_func(centers_dict.X, centers_dict.X)\n",
    "                                             + lam_new * np.diag(centers_dict.probs)))\n",
    "    U_DD, S_root_inv_DD = __stable_invert_root(U_DD, S_DD)\n",
    "    E = xp.asarray(S_root_inv_DD * U_DD.T)\n",
    "    # compute (X'X + lam*S^(-2))^(-1/2)XX'\n",
    "    X_precond = E.dot(K_DU)\n",
    "    # the diagonal entries of XX'(X'X + lam*S^(-2))^(-1)XX' are just the squared\n",
    "    # ell-2 norm of the columns of (X'X + lam*S^(-2))^(-1/2)XX'\n",
    "    tau = (diag_norm - xp.asnumpy(xp.square(X_precond, out=X_precond).sum(axis=0))) / lam_new\n",
    "    assert np.all(tau >= 0.), ('Some estimated RLS is negative, this should never happen. '\n",
    "                               'Min prob: {:.5f}'.format(np.min(tau)))\n",
    "    return tau\n",
    "\n",
    "def reduce_lambda(X: np.ndarray,\n",
    "                  similarity_func: callable,\n",
    "                  centers_dict: CentersDictionary,\n",
    "                  lam_new: float,\n",
    "                  random_state: np.random.RandomState,\n",
    "                  qbar=None,\n",
    "                  force_cpu=False):\n",
    "    n, d = X.shape\n",
    "    if qbar is None:\n",
    "        qbar = centers_dict.qbar\n",
    "    red_ratio = centers_dict.lam / lam_new\n",
    "    assert red_ratio >= 1.\n",
    "    diag = np.asarray(similarity_func.diag(X))\n",
    "    # compute upper confidence bound on RLS of each sample, overestimate (oversample) by a qbar factor\n",
    "    # to boost success probability at the expenses of a larger sample (dictionary)\n",
    "    ucb = np.minimum(qbar * diag / (diag + lam_new), 1.)\n",
    "    U = np.asarray(random_state.rand(n)) <= ucb\n",
    "    u = U.sum()\n",
    "    assert u > 0, ('No point selected during uniform sampling step, try to increase qbar. '\n",
    "                   'Expected number of points: {:.3f}'.format(n * ucb))\n",
    "    X_U = X[U, :]\n",
    "    # taus are RLS\n",
    "    tau = compute_tau(centers_dict, X_U, similarity_func, lam_new, force_cpu)\n",
    "    # RLS should always be smaller than 1\n",
    "    tau = np.minimum(tau, 1.)\n",
    "    # same as before, oversample by a qbar factor\n",
    "    probs = np.minimum(qbar * tau, ucb[U]) / ucb[U]\n",
    "    assert np.all(probs >= 0.), ('Some estimated probability is negative, this should never happen. '\n",
    "                                 'Min prob: {:.5f}'.format(np.min(probs)))\n",
    "    deff_estimate = probs.sum()/qbar\n",
    "    assert qbar*deff_estimate >= 1., ('Estimated deff is smaller than 1, you might want to reconsider your kernel. '\n",
    "                                      'deff_estimate: {:.3f}'.format(qbar*deff_estimate))\n",
    "    selected = np.asarray(random_state.rand(u)) <= probs\n",
    "    s = selected.sum()\n",
    "    assert s > 0, ('No point selected during RLS sampling step, try to increase qbar. '\n",
    "                   'Expected number of points (qbar*deff): {:.3f}'.format(np.sum(probs)))\n",
    "    D_new = CentersDictionary(idx=U.nonzero()[0][selected.nonzero()[0]],\n",
    "                              X=X_U[selected, :],\n",
    "                              probs=probs[selected],\n",
    "                              lam=lam_new,\n",
    "                              qbar=qbar)\n",
    "    return D_new\n",
    "\n",
    "def bless(X, similarity_func, lam_final=2.0, qbar=2, random_state=None, H=None, force_cpu=False, verbose=True):\n",
    "    n, d = X.shape\n",
    "    H = H if H is not None else np.ceil(np.log(n)).astype('int')\n",
    "    if random_state is None:\n",
    "        rng = np.random.RandomState()\n",
    "    elif isinstance(random_state, (int, int)):\n",
    "        rng = np.random.RandomState(random_state)\n",
    "    elif isinstance(random_state, np.random.RandomState):\n",
    "        rng = random_state\n",
    "    else:\n",
    "        raise ValueError('Cannot understand what you passed as a random number generator.')\n",
    "    diag_norm = np.asarray(similarity_func.diag(X))\n",
    "    ucb_init = qbar * diag_norm / n\n",
    "    selected_init = rng.rand(n) <= ucb_init\n",
    "    # force at least one sample to be selected\n",
    "    selected_init[0] = 1\n",
    "    D = CentersDictionary(idx=selected_init.nonzero(),\n",
    "                   X=X[selected_init, :],\n",
    "                   probs=np.ones(np.sum(selected_init)) * ucb_init[selected_init],\n",
    "                   lam=n,\n",
    "                   qbar=qbar)\n",
    "    lam_sequence = list(np.geomspace(lam_final, n, H))\n",
    "    # discard n from the list, we already used it to initialize\n",
    "    lam_sequence.pop()\n",
    "    with __get_progress_bar(total=len(lam_sequence), disable=not(verbose)) as t:\n",
    "        while len(lam_sequence) > 0:\n",
    "            lam_new = lam_sequence.pop()\n",
    "            D = reduce_lambda(X, similarity_func, D, lam_new, rng, force_cpu=force_cpu)\n",
    "            t.set_postfix(lam=int(lam_new),\n",
    "                          m=len(D.probs),\n",
    "                          m_expected=int(D.probs.mean()*n),\n",
    "                          probs_dist=f\"({D.probs.mean()/qbar:.4}, {D.probs.max()/qbar:.4}, {D.probs.min()/qbar:.4})\")\n",
    "            t.update()\n",
    "    return D\n",
    "\n",
    "def get_nystrom_embeddings(X, centers_dict, similarity_func, force_cpu=False):\n",
    "    xp = __load_gpu_module(force_cpu)\n",
    "    K_XD = xp.asarray(similarity_func(X, centers_dict.X))\n",
    "    U_DD, S_DD, _ = np.linalg.svd(xp.asnumpy(similarity_func(centers_dict.X, centers_dict.X)))\n",
    "    U_DD, S_root_inv_DD = __stable_invert_root(U_DD, S_DD)\n",
    "    K_DD_inv_sqrt = xp.asarray(U_DD * S_root_inv_DD.T)\n",
    "    return K_XD.dot(K_DD_inv_sqrt)\n",
    "def get_nystrom_matrix_approx(X, centers_dict, similarity_func, force_cpu=False):\n",
    "    B = get_nystrom_embeddings(X, centers_dict, similarity_func, force_cpu)\n",
    "    return B.dot(B.T)\n",
    "def get_nystrom_PCA(X, centers_dict, similarity_func, k=-1, force_cpu=False):\n",
    "    B = get_nystrom_embeddings(X, centers_dict, similarity_func, force_cpu)\n",
    "    if k > B.shape[1]:\n",
    "        raise ValueError('requesting k={} principal components, but the centers dictionary can only'\n",
    "                         'approximate m={} components.'.format(k, B.shape[1]))\n",
    "    U, Sigma, _ = np.linalg.svd(B,\n",
    "                                full_matrices=False,\n",
    "                                compute_uv=True)\n",
    "    return np.dot(U, np.diag(Sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9: lam=10 m=380 m_expected=566 probs_dist=(0.001889, 0.007065, 0.000445)      \n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF\n",
    "X_test = np.random.randn(30000, 10)\n",
    "r = np.random.RandomState(42)\n",
    "D_test = bless(X_test, RBF(length_scale=10), 10, 10, r, 10, force_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0, 72], dtype=int64),)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF\n",
    "similarity_func = RBF(length_scale=10)\n",
    "# Create a dataset X\n",
    "X = np.random.rand(100, 20)\n",
    "n, d = X.shape\n",
    "diag_norm = np.asarray(similarity_func.diag(X)) # it gives the diagonal of the kernel matrix = 1\n",
    "qbar=2\n",
    "ucb_init = qbar * diag_norm / n\n",
    "rng = np.random.RandomState(42)\n",
    "selected_init = rng.rand(n) <= ucb_init\n",
    "# force at least one sample to be selected\n",
    "selected_init[0] = 1\n",
    "D = CentersDictionary(idx=selected_init.nonzero(),\n",
    "                X=X[selected_init, :],\n",
    "                probs=np.ones(np.sum(selected_init)) * ucb_init[selected_init],\n",
    "                lam=n,\n",
    "                qbar=qbar)\n",
    "lam_final = 10;H =10\n",
    "lam_sequence = list(np.geomspace(lam_final, n, H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cupy not found, defaulting to numpy\n",
      "Tau estimates:\n",
      " [0.01781756 0.13935384 0.14266185 0.11074688 0.11260559 0.0886903\n",
      " 0.22677102 0.16531355 0.12992787 0.17682328 0.13984242 0.12395724\n",
      " 0.13961938 0.13813203 0.14040698 0.15188574 0.18981059 0.16841886\n",
      " 0.14274827 0.11090436 0.14115638 0.10050757 0.14082007 0.17502418\n",
      " 0.16917262 0.12885603 0.1346672  0.17618711 0.10901972 0.10625629\n",
      " 0.11069173 0.13687716 0.12374111 0.17275162 0.09592449 0.10571218\n",
      " 0.11415503 0.11081151 0.11795866 0.14704404 0.0849966  0.12746826\n",
      " 0.13138316 0.15315141 0.14677184 0.13325706 0.14712573 0.09551545\n",
      " 0.15996744 0.16137679 0.09633379 0.1022168  0.14941639 0.12413216\n",
      " 0.13176019 0.12759969 0.18369668 0.14822926 0.08636498 0.09541002\n",
      " 0.15641286 0.08660714 0.11272029 0.0932371  0.10120348 0.136261\n",
      " 0.1151679  0.14085564 0.09996032 0.12763398 0.11688655 0.13793836\n",
      " 0.01781756 0.11703629 0.17414882 0.11799593 0.17266841 0.14033381\n",
      " 0.17159034 0.13366023 0.14340924 0.15101238 0.15433955 0.12965325\n",
      " 0.13435802 0.11028859 0.15821955 0.10042003 0.08970963 0.08418811\n",
      " 0.0980339  0.17529156 0.07431837 0.18853986 0.09701734 0.14329687\n",
      " 0.11995118 0.09984947 0.10911615 0.14997715]\n"
     ]
    }
   ],
   "source": [
    "# Compute the estimates of all RLS using the compute_tau function\n",
    "lam_new = 0.2\n",
    "force_cpu = False\n",
    "tau_estimates = compute_tau(D, X, similarity_func, lam_new, force_cpu)\n",
    "print(\"Tau estimates:\\n\", tau_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "def leverage_scores_sampling(A, k):\n",
    "    # compute the SVD of A\n",
    "    U, s, Vt = svd(A, full_matrices=False)\n",
    "    # compute the leverage scores\n",
    "    leverage_scores = np.sum(U**2, axis=0)\n",
    "    # normalize the scores\n",
    "    p = leverage_scores / np.sum(leverage_scores)\n",
    "    # sample k columns with replacement according to p\n",
    "    indices = np.random.choice(A.shape[1], size=k, replace=False, p=p)\n",
    "    # construct the sampled matrix\n",
    "    B = A[:, indices]\n",
    "    return B\n",
    "\n",
    "# Example usage\n",
    "A = np.random.rand(10, 10)\n",
    "k = 5\n",
    "B = leverage_scores_sampling(A, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60993489, 0.92288084, 0.93574355, 0.19494487, 0.97382232],\n",
       "       [0.94025987, 0.18510137, 0.55533147, 0.58196297, 0.75512147],\n",
       "       [0.72093542, 0.48480624, 0.94979285, 0.9043004 , 0.13503545],\n",
       "       [0.5684627 , 0.60500483, 0.95821358, 0.19115622, 0.03347619],\n",
       "       [0.22509391, 0.10791163, 0.1243677 , 0.06702738, 0.97213703],\n",
       "       [0.3843179 , 0.44333009, 0.44619953, 0.53157502, 0.39892993],\n",
       "       [0.3495867 , 0.0983109 , 0.6739625 , 0.47256958, 0.6333704 ],\n",
       "       [0.86184064, 0.47377077, 0.49502417, 0.60095499, 0.4892662 ],\n",
       "       [0.24085047, 0.68571423, 0.68495285, 0.12538915, 0.36963362],\n",
       "       [0.64057427, 0.28317572, 0.67757728, 0.44232227, 0.30384955]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
