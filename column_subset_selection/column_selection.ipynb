{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "CentersDictionary = namedtuple('CentersDictionary', ('idx', 'X', 'probs', 'lam', 'qbar'))\n",
    "\n",
    "def __load_gpu_module(force_cpu: bool):\n",
    "    xp = np\n",
    "    xp.asnumpy = np.asarray\n",
    "    if not force_cpu:\n",
    "        try:\n",
    "            import cupy as cp\n",
    "            xp = cp\n",
    "        except ImportError:\n",
    "            print(\"cupy not found, defaulting to numpy\")\n",
    "    return xp\n",
    "\n",
    "def __stable_invert_root(U: np.ndarray, S: np.ndarray):\n",
    "    n = U.shape[0]\n",
    "    assert U.shape == (n, n)\n",
    "    assert S.shape == (n,)\n",
    "    # threshold formula taken from pinv2's implementation of numpy/scipy\n",
    "    thresh = S.max() * max(S.shape) * np.finfo(S.dtype).eps\n",
    "    stable_eig = np.logical_not(np.isclose(S, 0., atol=thresh))\n",
    "    m = sum(stable_eig)\n",
    "    U_thin = U[:, stable_eig]\n",
    "    S_thin = S[stable_eig]\n",
    "    assert U_thin.shape == (n, m)\n",
    "    assert S_thin.shape == (m,)\n",
    "    S_thin_inv_root = (1 / np.sqrt(S_thin)).reshape(-1, 1)\n",
    "    return U_thin, S_thin_inv_root\n",
    "\n",
    "\n",
    "def compute_tau(centers_dict: CentersDictionary,\n",
    "                X: np.ndarray,\n",
    "                similarity_func: callable,\n",
    "                lam_new: float,\n",
    "                force_cpu=False):\n",
    "    xp = __load_gpu_module(force_cpu)\n",
    "    diag_norm = np.asarray(similarity_func.diag(X))\n",
    "    # (m x n) kernel matrix between samples in dictionary and dataset X\n",
    "    K_DU = xp.asarray(similarity_func(centers_dict.X, X))\n",
    "    # the estimator proposed in Calandriello et al. 2017 is\n",
    "    # diag(XX' - XX'S(SX'XS + lam*I)^(-1)SXX')/lam\n",
    "    # here for efficiency we collect an S inside the inverse and compute\n",
    "    # diag(XX' - XX'(X'X + lam*S^(-2))^(-1)XX')/lam\n",
    "    # note that in the second term we take care of dropping the rows/columns of X associated\n",
    "    # with 0 entries in S\n",
    "    U_DD, S_DD, _ = np.linalg.svd(xp.asnumpy(similarity_func(centers_dict.X, centers_dict.X)\n",
    "                                             + lam_new * np.diag(centers_dict.probs)))\n",
    "    U_DD, S_root_inv_DD = __stable_invert_root(U_DD, S_DD)\n",
    "    E = xp.asarray(S_root_inv_DD * U_DD.T)\n",
    "    # compute (X'X + lam*S^(-2))^(-1/2)XX'\n",
    "    \n",
    "    # TODO: try to understand this \n",
    "    X_precond = E.dot(K_DU)\n",
    "    # the diagonal entries of XX'(X'X + lam*S^(-2))^(-1)XX' are just the squared\n",
    "    # ell-2 norm of the columns of (X'X + lam*S^(-2))^(-1/2)XX'\n",
    "    tau = (diag_norm - xp.asnumpy(xp.square(X_precond, out=X_precond).sum(axis=0))) / lam_new\n",
    "    assert np.all(tau >= 0.), ('Some estimated RLS is negative, this should never happen. '\n",
    "                               'Min prob: {:.5f}'.format(np.min(tau)))\n",
    "    return tau"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For any symmetric matrix $A\\in\\mathbb R^{n\\times m}$ and any $\\gamma>0$, $$A(AA^T+\\gamma I_m)^{-1}A^T=AA^T(AA^T+\\gamma I_m)^{-1}$$\n",
    "- For any symmetric matrix $A\\in\\mathbb R^{n\\times n}$ and diagonal matrix $B\\in\\mathbb R^{n\\times n}$ such that $B$ has $n-s$ zero entries, and $s$ non-zero enties, define $C\\in \\mathbb R^{n\\times s}$ as the matrix obtained by removing all zero columns in $B$. Then, $$AB(BAB+\\gamma I_m)^{-1}BA=AC(C^TAC+\\gamma I_s)^{-1}C^TA$$\n",
    "- For any appropriately shaped matrix $A, B, C$, with $A$ and $B$ invertible, the Woodbury matrix identity states, $$(A+CBC^T)^{-1}=A^{-1}-A^{-1}C(C^TA^{-1}C+B^{-1})^{-1}C^TA^{-1}$$\n",
    "\n",
    "Source: Distributed Adaptive Sampling for Kernel Matrix Approximation, Calandriello, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lambda(X: np.ndarray,\n",
    "                  similarity_func: callable,\n",
    "                  centers_dict: CentersDictionary,\n",
    "                  lam_new: float,\n",
    "                  random_state: np.random.RandomState,\n",
    "                  qbar=None,\n",
    "                  force_cpu=False):\n",
    "    n, d = X.shape\n",
    "    if qbar is None:\n",
    "        qbar = centers_dict.qbar\n",
    "    red_ratio = centers_dict.lam / lam_new\n",
    "    assert red_ratio >= 1.\n",
    "    diag = np.asarray(similarity_func.diag(X))\n",
    "    # compute upper confidence bound on RLS of each sample, overestimate (oversample) by a qbar factor\n",
    "    # to boost success probability at the expenses of a larger sample (dictionary)\n",
    "    ucb = np.minimum(qbar * diag / (diag + lam_new), 1.)\n",
    "    U = np.asarray(random_state.rand(n)) <= ucb\n",
    "    u = U.sum()\n",
    "    assert u > 0, ('No point selected during uniform sampling step, try to increase qbar. '\n",
    "                   'Expected number of points: {:.3f}'.format(n * ucb))\n",
    "    X_U = X[U, :]\n",
    "    # taus are RLS\n",
    "    tau = compute_tau(centers_dict, X_U, similarity_func, lam_new, force_cpu)\n",
    "    # RLS should always be smaller than 1\n",
    "    tau = np.minimum(tau, 1.)\n",
    "    # same as before, oversample by a qbar factor\n",
    "    probs = np.minimum(qbar * tau, ucb[U]) / ucb[U]\n",
    "    assert np.all(probs >= 0.), ('Some estimated probability is negative, this should never happen. '\n",
    "                                 'Min prob: {:.5f}'.format(np.min(probs)))\n",
    "    deff_estimate = probs.sum()/qbar\n",
    "    assert qbar*deff_estimate >= 1., ('Estimated deff is smaller than 1, you might want to reconsider your kernel. '\n",
    "                                      'deff_estimate: {:.3f}'.format(qbar*deff_estimate))\n",
    "    selected = np.asarray(random_state.rand(u)) <= probs\n",
    "    s = selected.sum()\n",
    "    assert s > 0, ('No point selected during RLS sampling step, try to increase qbar. '\n",
    "                   'Expected number of points (qbar*deff): {:.3f}'.format(np.sum(probs)))\n",
    "    D_new = CentersDictionary(idx=U.nonzero()[0][selected.nonzero()[0]],\n",
    "                              X=X_U[selected, :],\n",
    "                              probs=probs[selected],\n",
    "                              lam=lam_new,\n",
    "                              qbar=qbar)\n",
    "    return D_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_progress_bar(total=-1, disable=False):\n",
    "    \"\"\"Helper function to get a tqdm progress bar (or a simple fallback otherwise)\"\"\"\n",
    "    class ProgBar(object):\n",
    "        def __init__(self, total=-1, disable=False):\n",
    "            self.disable = disable\n",
    "            self.t = 0\n",
    "            self.total = total\n",
    "            self.debug_string = \"\"\n",
    "\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "\n",
    "        def __exit__(self, *args, **kwargs):\n",
    "            pass\n",
    "\n",
    "        def set_postfix(self, **kwargs):\n",
    "            self.debug_string = \"\"\n",
    "            for arg in kwargs:\n",
    "                self.debug_string += \"{}={} \".format(arg, kwargs[arg])\n",
    "\n",
    "        def update(self):\n",
    "            if not self.disable:\n",
    "                self.t += 1\n",
    "                print_str = \"{}\".format(self.t)\n",
    "\n",
    "                if self.total > 0:\n",
    "                    print_str += \"/{}\".format(self.total)\n",
    "\n",
    "                print_str += \": {}\".format(self.debug_string)\n",
    "\n",
    "                if len(print_str) < 80:\n",
    "                    print_str = print_str + \" \"*(80 - len(print_str))\n",
    "\n",
    "                print(print_str, end='\\r', flush=True)\n",
    "\n",
    "            if self.t == self.total:\n",
    "                print(\"\")\n",
    "\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "        progress_bar = tqdm(total=total, disable=disable)\n",
    "    except ImportError:\n",
    "        progress_bar = ProgBar(total=total, disable=disable)\n",
    "\n",
    "    return progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bless(X, similarity_func, lam_final=2.0, qbar=2, random_state=None, H=None, force_cpu=False, verbose=True):\n",
    "    n, d = X.shape\n",
    "    H = H if H is not None else np.ceil(np.log(n)).astype('int')\n",
    "    if random_state is None:\n",
    "        rng = np.random.RandomState()\n",
    "    elif isinstance(random_state, (int, int)):\n",
    "        rng = np.random.RandomState(random_state)\n",
    "    elif isinstance(random_state, np.random.RandomState):\n",
    "        rng = random_state\n",
    "    else:\n",
    "        raise ValueError('Cannot understand what you passed as a random number generator.')\n",
    "    diag_norm = np.asarray(similarity_func.diag(X))\n",
    "    ucb_init = qbar * diag_norm / n\n",
    "    selected_init = rng.rand(n) <= ucb_init\n",
    "    # force at least one sample to be selected\n",
    "    selected_init[0] = 1\n",
    "    D = CentersDictionary(idx=selected_init.nonzero(),\n",
    "                   X=X[selected_init, :],\n",
    "                   probs=np.ones(np.sum(selected_init)) * ucb_init[selected_init],\n",
    "                   lam=n,\n",
    "                   qbar=qbar)\n",
    "\n",
    "    lam_sequence = list(np.geomspace(lam_final, n, H))\n",
    "    # discard n from the list, we already used it to initialize\n",
    "    lam_sequence.pop()\n",
    "    # print total number of points selected\n",
    "    print(\"Selected points: {}\".format(D.idx))\n",
    "    with __get_progress_bar(total=len(lam_sequence), disable=not(verbose)) as t:\n",
    "        while len(lam_sequence) > 0:\n",
    "            lam_new = lam_sequence.pop()\n",
    "            D = reduce_lambda(X, similarity_func, D, lam_new, rng, force_cpu=force_cpu)\n",
    "            print(\"Selected points: {}\".format(D.idx))\n",
    "            t.set_postfix(lam=int(lam_new),\n",
    "                          m=len(D.probs),\n",
    "                          m_expected=int(D.probs.mean()*n),\n",
    "                          probs_dist=f\"({D.probs.mean()/qbar:.4}, {D.probs.max()/qbar:.4}, {D.probs.min()/qbar:.4})\")\n",
    "            t.update()\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected points: (array([    0,  2604,  2902,  4151,  4725,  7007,  8125,  9187,  9907,\n",
      "       11883, 15714, 16001, 16560, 21932, 21945, 25861, 26008, 26863,\n",
      "       26950, 27240], dtype=int64),)\n",
      "Selected points: [12631 17227 19532 21552 22154 22644]\n",
      "Selected points: [  858  2291  2696  3529  5072  5628  5829  7015  7283  7874  7875  8467\n",
      "  9399  9766 10172 10704 11506 11960 12374 12535 13262 14204 14972 15395\n",
      " 16163 16836 17393 17741 17957 18756 19304 19448 19463 19763 19798 20303\n",
      " 20698 21976 22356 23346 23639 23693 24144 25352 25568 25622 26419 27527\n",
      " 28603 29225 29672]\n",
      "Selected points: [   70   172   310   437   581   867  1199  1243  1265  1296  1617  2267\n",
      "  2347  2411  2583  2621  3194  3217  3440  3504  3533  3539  3709  3788\n",
      "  4046  4261  4516  4737  5130  5565  5665  6165  6483  6600  7022  7452\n",
      "  7613  7652  8096  8164  8209  8276  8282  8483  8594  8769  8777  9247\n",
      "  9611  9742  9747  9793 10164 10267 10434 10458 10504 10844 11109 11176\n",
      " 11633 11752 11912 12392 12434 12480 12646 12652 12712 12929 13159 13166\n",
      " 13212 13485 13665 13927 14295 14328 14413 14770 15279 15492 15692 16060\n",
      " 16511 16589 16701 16799 17029 17165 17416 17817 18141 18309 18880 19001\n",
      " 19128 19320 19393 19516 19567 20243 20793 20979 21017 21040 21244 21248\n",
      " 21300 21315 21336 21690 22011 22026 22096 22257 22655 23296 23312 23530\n",
      " 23742 23790 24196 24470 24538 24783 24896 25501 25619 25994 26310 26425\n",
      " 26463 27072 27272 27710 27809 27868 27872 28276 28369 28991 29072 29701\n",
      " 29735 29916]\n",
      "Selected points: [  304   414   542   578   593   698   738   864   872  1133  1142  1268\n",
      "  1302  1335  1371  1576  1656  1803  1862  1936  1983  2012  2086  2109\n",
      "  2161  2189  2192  2228  2287  2400  2403  2443  2556  2706  2761  2767\n",
      "  2850  2908  2915  3068  3114  3326  3446  3456  3482  3598  3841  3941\n",
      "  3962  4004  4028  4159  4175  4244  4247  4315  4487  4524  4623  4635\n",
      "  4751  4877  4903  4919  4939  5005  5082  5092  5132  5365  5513  5601\n",
      "  5631  5720  5984  6017  6090  6148  6206  6243  6403  6414  6440  6499\n",
      "  6554  6755  6775  6798  6821  6858  6970  7282  7343  7376  7490  7561\n",
      "  7576  7688  7709  7803  7960  7975  8030  8040  8087  8182  8782  8822\n",
      "  8868  8939  8956  9163  9360  9385  9571  9598  9756  9802  9810  9812\n",
      "  9817  9822  9927 10069 10079 10101 10183 10303 10526 10707 10754 10765\n",
      " 10770 10904 11180 11514 11628 11677 11752 11881 12011 12097 12165 12410\n",
      " 12639 12700 12947 12985 13027 13094 13222 13269 13330 13446 13464 13526\n",
      " 13870 14038 14066 14204 14254 14398 14484 14495 14558 14625 14745 14960\n",
      " 15433 15537 15546 15731 15852 16023 16140 16163 16360 16371 16530 16532\n",
      " 16646 16693 16829 16904 16938 17074 17129 17146 17474 17484 17492 17696\n",
      " 17749 17757 17939 18085 18162 18319 18711 18713 18835 19006 19037 19164\n",
      " 19269 19344 19349 19382 19408 19494 19644 19754 19897 20083 20134 20156\n",
      " 20247 20301 20570 20636 20723 20876 21027 21029 21058 21241 21288 21289\n",
      " 21466 21490 21787 21831 21990 22072 22205 22257 22278 22413 22788 22830\n",
      " 22893 23301 23404 23455 23592 23612 23642 23815 23905 23935 24001 24128\n",
      " 24234 24328 24408 24561 24618 24678 24896 25277 25405 25569 25573 25648\n",
      " 26101 26103 26318 26379 26410 26621 26812 26968 27008 27114 27152 27178\n",
      " 27179 27211 27243 27450 27581 27582 27731 27740 27834 27837 27901 28002\n",
      " 28019 28114 28321 28715 29013 29025 29042 29107 29272 29292 29503 29522\n",
      " 29706 29844 29915 29925]\n",
      "Selected points: [   53    76    79   161   210   332   428   519   620   700   730   816\n",
      "   824   882   896  1065  1136  1205  1270  1498  1503  1523  1549  1594\n",
      "  1633  1648  1835  2041  2085  2157  2172  2175  2208  2292  2371  2502\n",
      "  2596  2839  2858  2933  2977  3007  3018  3027  3070  3154  3175  3193\n",
      "  3217  3360  3400  3459  3476  3491  3506  3515  3576  3618  3699  3799\n",
      "  3829  3920  4042  4070  4133  4213  4215  4241  4265  4343  4383  4614\n",
      "  4761  4766  5019  5128  5238  5415  5565  5602  5696  5760  5954  5957\n",
      "  6062  6122  6134  6314  6406  6411  6511  6682  6683  6695  6838  6889\n",
      "  6961  7157  7168  7175  7192  7311  7352  7411  7452  7511  7546  7733\n",
      "  7839  7864  7905  7909  7913  7917  7948  8017  8022  8070  8200  8250\n",
      "  8277  8391  8450  8629  8638  8869  9021  9022  9064  9089  9103  9229\n",
      "  9290  9450  9503  9543  9588  9627  9640  9717  9733  9804  9898  9948\n",
      " 10023 10062 10068 10070 10132 10157 10329 10367 10387 10401 10461 10492\n",
      " 10505 10525 10580 10583 10616 10636 10655 10678 10681 10694 10875 10922\n",
      " 11038 11041 11074 11092 11129 11204 11207 11351 11357 11481 11559 11605\n",
      " 11689 11783 11892 11914 12085 12102 12114 12121 12205 12236 12248 12354\n",
      " 12529 12532 12653 12701 12767 12915 12929 12952 13071 13269 13284 13291\n",
      " 13379 13398 13498 13504 13540 13641 13705 13754 13768 13840 13858 13907\n",
      " 14033 14167 14203 14395 14462 14516 14593 14617 14629 14682 14725 14728\n",
      " 14818 14872 14938 14958 15019 15066 15111 15156 15166 15199 15321 15332\n",
      " 15338 15372 15393 15485 15537 15601 15603 15764 15769 15811 15957 15969\n",
      " 15990 16092 16251 16259 16419 16487 16527 16638 16708 16710 16720 16818\n",
      " 16890 16947 17051 17175 17340 17536 17597 17649 17673 17781 17826 17879\n",
      " 17955 18043 18065 18177 18220 18260 18286 18390 18399 18401 18411 18445\n",
      " 18509 18568 18607 18714 18731 18779 18853 18861 18984 18999 19006 19065\n",
      " 19083 19091 19123 19191 19233 19248 19424 19509 19617 19642 19704 19736\n",
      " 19760 19960 20013 20039 20054 20075 20194 20247 20277 20328 20596 20778\n",
      " 20881 21033 21105 21205 21231 21238 21322 21553 21757 21765 21799 21885\n",
      " 21970 22009 22259 22327 22426 22470 22498 22499 22508 22552 22597 22602\n",
      " 22702 22711 22782 22856 22916 22931 22950 23004 23017 23064 23098 23209\n",
      " 23225 23327 23367 23465 23578 23614 23685 23707 23723 23737 23754 23778\n",
      " 23815 23830 23860 23927 23931 23945 23970 24030 24051 24149 24177 24205\n",
      " 24231 24245 24279 24420 24533 24619 24647 24663 24666 24911 24983 25003\n",
      " 25045 25117 25124 25150 25173 25247 25269 25287 25289 25349 25375 25400\n",
      " 25442 25445 25498 25542 25593 25606 25642 25744 25843 25848 25987 26067\n",
      " 26124 26191 26232 26258 26311 26312 26319 26321 26375 26396 26434 26442\n",
      " 26588 26625 26643 26668 26802 26833 26922 26925 27043 27150 27161 27208\n",
      " 27439 27445 27463 27472 27638 27647 27656 27912 27958 28089 28210 28306\n",
      " 28376 28433 28455 28558 28563 28617 28732 28756 28772 28773 28909 29052\n",
      " 29109 29125 29327 29420 29522 29564 29629 29718 29739 29816 29850 29950\n",
      " 29975]\n",
      "Selected points: [   29    37   138   261   282   292   400   410   448   522   543   546\n",
      "   628   664   685   695   743   758   816   891   966   986  1091  1186\n",
      "  1206  1215  1258  1330  1422  1476  1514  1555  1585  1603  1802  1976\n",
      "  1989  2003  2109  2113  2135  2220  2223  2275  2293  2316  2361  2427\n",
      "  2430  2461  2481  2522  2584  2593  2626  2632  2723  2835  2855  3182\n",
      "  3384  3413  3455  3505  3527  3533  3716  3796  3831  3851  3878  3884\n",
      "  4224  4274  4288  4362  4388  4393  4448  4455  4496  4520  4537  4620\n",
      "  4625  4644  4678  4699  4702  4717  4787  4807  4846  4969  4999  5101\n",
      "  5103  5124  5247  5337  5348  5352  5371  5659  5712  5737  5786  5964\n",
      "  5982  6004  6023  6054  6194  6215  6311  6597  6614  6704  6718  6765\n",
      "  7083  7146  7312  7435  7469  7631  7658  7872  7928  8011  8041  8076\n",
      "  8149  8189  8235  8287  8289  8326  8344  8359  8436  8534  8664  8671\n",
      "  8710  8713  8859  9046  9062  9095  9363  9400  9412  9447  9549  9574\n",
      "  9650  9655  9674  9808  9899 10004 10171 10246 10255 10341 10357 10436\n",
      " 10631 10792 10821 10839 10928 10935 11042 11059 11082 11279 11329 11388\n",
      " 11610 11685 11731 11829 11926 11948 11990 12070 12358 12409 12411 12428\n",
      " 12509 12514 12550 12601 12602 12605 12609 12614 12631 12641 12714 12947\n",
      " 12958 13159 13308 13353 13366 13475 13594 13616 13677 13716 13752 13820\n",
      " 13867 13978 13985 14009 14132 14168 14174 14277 14291 14447 14474 14544\n",
      " 14559 14616 14657 14664 14695 14752 14839 15011 15055 15112 15253 15359\n",
      " 15461 15520 15552 15711 15759 15778 16012 16018 16063 16076 16114 16197\n",
      " 16222 16285 16377 16446 16448 16451 16548 16687 16725 16786 16818 16831\n",
      " 16956 17079 17104 17120 17264 17299 17355 17390 17515 17646 17650 17668\n",
      " 17694 17870 17965 17977 17999 18039 18058 18130 18224 18256 18276 18296\n",
      " 18363 18540 18582 18616 18642 18742 18773 18823 18828 18911 18934 18961\n",
      " 19002 19019 19024 19145 19213 19258 19312 19437 19464 19523 19585 19764\n",
      " 19784 19827 19906 19940 19941 19965 20023 20099 20298 20353 20373 20377\n",
      " 20495 20580 20593 20647 20649 20694 20701 20792 20850 20883 20967 21016\n",
      " 21080 21098 21288 21451 21456 21577 21620 21667 21681 21797 21951 21966\n",
      " 21982 22015 22019 22091 22138 22141 22210 22224 22258 22295 22308 22320\n",
      " 22368 22484 22527 22807 22852 22974 22986 22995 22999 23130 23226 23444\n",
      " 23484 23529 23599 23764 23777 23937 24072 24094 24229 24259 24303 24317\n",
      " 24362 24367 24377 24437 24438 24508 24740 24841 24847 24958 24985 25014\n",
      " 25112 25123 25147 25163 25243 25294 25322 25372 25558 25621 25642 25811\n",
      " 25816 25845 26147 26175 26233 26297 26314 26353 26380 26541 26544 26585\n",
      " 26716 26807 26966 26980 27007 27026 27085 27086 27097 27100 27106 27211\n",
      " 27215 27300 27399 27417 27421 27438 27471 27500 27521 27698 27741 27743\n",
      " 27970 28157 28183 28263 28283 28345 28403 28406 28487 28524 28587 28697\n",
      " 28756 28766 28830 28955 28999 29054 29126 29162 29166 29168 29187 29199\n",
      " 29280 29303 29369 29406 29416 29507 29553 29598 29715 29777 29810 29948]\n",
      "Selected points: [   11    50   142   267   346   401   424   538   542   563   567   614\n",
      "   677   779   804   832   849   995  1000  1027  1254  1284  1345  1560\n",
      "  1586  1604  1715  1733  1799  1834  1873  1892  1925  2012  2027  2064\n",
      "  2125  2236  2246  2269  2293  2327  2387  2445  2550  2589  2623  2625\n",
      "  2652  2672  2697  2768  2890  2940  2941  2978  2991  3061  3073  3096\n",
      "  3208  3337  3355  3563  3628  3734  3768  3782  3834  3869  3878  3982\n",
      "  4010  4225  4240  4245  4291  4346  4430  4527  4613  4619  4699  4723\n",
      "  4737  4803  4828  4887  4959  5119  5146  5148  5220  5350  5652  5702\n",
      "  5755  5772  5846  5869  5873  5891  5939  5964  6004  6022  6026  6103\n",
      "  6125  6193  6231  6289  6318  6480  6544  6594  6817  6873  6970  7038\n",
      "  7084  7148  7156  7267  7283  7293  7309  7443  7483  7547  7608  7611\n",
      "  7709  7747  7768  7818  7855  7975  8035  8105  8175  8224  8337  8406\n",
      "  8460  8529  8530  8532  8541  8549  8650  8734  8775  8857  9051  9153\n",
      "  9186  9270  9353  9383  9468  9480  9498  9698  9699  9715  9892  9898\n",
      "  9984 10035 10059 10113 10168 10275 10324 10400 10446 10542 10724 10745\n",
      " 10853 10893 10902 10913 11053 11100 11103 11206 11229 11236 11244 11288\n",
      " 11344 11395 11474 11501 11674 11678 11695 11697 11781 11943 11976 12027\n",
      " 12031 12204 12251 12304 12339 12375 12413 12526 12555 12597 12610 12630\n",
      " 12742 12751 12778 12835 12841 12915 12927 12966 12973 12980 13012 13192\n",
      " 13222 13428 13514 13515 13538 13568 13569 13599 13634 13721 13753 13800\n",
      " 13838 14006 14031 14044 14077 14107 14111 14117 14183 14222 14227 14323\n",
      " 14359 14371 14381 14383 14434 14492 14564 14607 14707 14765 14790 14815\n",
      " 14885 14929 14950 15015 15093 15140 15146 15155 15187 15313 15372 15377\n",
      " 15501 15510 15550 15619 15627 15634 15748 15776 15837 15839 15856 15934\n",
      " 15951 15991 16047 16098 16119 16171 16229 16236 16355 16417 16503 16504\n",
      " 16556 16586 16599 16694 16820 16955 17034 17105 17190 17291 17463 17540\n",
      " 17611 17634 17664 17673 17709 17756 17759 17806 17982 18005 18069 18162\n",
      " 18217 18221 18256 18302 18551 18618 18625 18657 18687 18697 18774 18825\n",
      " 18827 18858 18883 18887 19033 19046 19087 19123 19154 19193 19241 19328\n",
      " 19383 19401 19475 19486 19508 19590 19639 19663 19670 19674 19729 19803\n",
      " 19849 19885 19898 19981 20093 20173 20198 20234 20235 20374 20475 20638\n",
      " 20639 20662 20690 20757 20796 20798 20905 21017 21027 21121 21131 21275\n",
      " 21282 21353 21377 21457 21482 21595 21646 21675 21849 22054 22148 22271\n",
      " 22276 22354 22393 22461 22551 22690 22702 22778 22905 22909 22926 22931\n",
      " 22979 23011 23027 23069 23265 23310 23638 23669 23839 23901 23929 23932\n",
      " 23952 24064 24068 24088 24099 24224 24235 24572 24577 24622 24654 24689\n",
      " 24696 24711 24810 24923 24954 25074 25078 25080 25103 25123 25229 25265\n",
      " 25440 25537 25635 25899 25907 25913 25923 26037 26078 26089 26163 26177\n",
      " 26264 26285 26286 26323 26326 26427 26477 26706 26733 26792 26841 26943\n",
      " 26949 27024 27049 27090 27092 27100 27137 27138 27162 27272 27275 27307\n",
      " 27320 27436 27511 27534 27651 27655 27706 27780 27785 27853 27938 27963\n",
      " 28096 28193 28252 28304 28335 28362 28393 28559 28604 28609 28638 28641\n",
      " 28651 28759 28802 28939 28975 29023 29033 29070 29075 29150 29175 29179\n",
      " 29181 29294 29334 29347 29361 29363 29477 29667 29672 29862 29863 29895\n",
      " 29932 29957]\n",
      "Selected points: [   25    92   134   145   207   280   325   343   456   466   620   636\n",
      "   646   671   700   843   897   989  1010  1050  1091  1147  1199  1233\n",
      "  1401  1425  1495  1712  1818  1835  1887  2020  2102  2174  2239  2277\n",
      "  2322  2339  2357  2399  2472  2493  2593  2733  2760  2809  2831  2841\n",
      "  2943  3079  3085  3097  3112  3201  3219  3369  3393  3530  3571  3589\n",
      "  3634  3677  3694  3699  3727  3741  3844  3889  3926  3940  3975  3979\n",
      "  3993  4019  4069  4152  4170  4172  4211  4292  4338  4339  4379  4393\n",
      "  4497  4521  4556  4594  4677  5004  5031  5070  5093  5099  5102  5276\n",
      "  5282  5323  5387  5427  5442  5458  5472  5615  5654  5669  5690  5751\n",
      "  5788  5797  5985  5990  6037  6090  6098  6143  6198  6217  6259  6375\n",
      "  6454  6458  6543  6550  6749  6812  6919  6994  7042  7136  7153  7168\n",
      "  7224  7267  7279  7292  7318  7438  7549  7578  7594  7601  7683  7839\n",
      "  7908  8014  8022  8085  8098  8136  8273  8365  8431  8493  8538  8589\n",
      "  8598  8763  8774  8788  8971  9163  9213  9223  9269  9348  9381  9396\n",
      "  9457  9481  9503  9515  9524  9573  9602  9605  9638  9653  9741  9744\n",
      "  9873  9894  9941  9949  9997 10013 10106 10176 10235 10411 10495 10500\n",
      " 10617 10648 10681 10789 10800 10837 10945 11128 11216 11275 11293 11340\n",
      " 11429 11453 11511 11538 11670 11688 11719 11730 11766 11822 11937 11952\n",
      " 11954 12042 12065 12122 12169 12204 12294 12342 12347 12460 12614 12625\n",
      " 12711 12725 12891 12950 13167 13203 13221 13305 13310 13313 13323 13337\n",
      " 13420 13542 13673 13739 13758 13759 13880 13931 14070 14103 14218 14394\n",
      " 14405 14409 14481 14517 14539 14585 14615 14637 14703 14841 15037 15149\n",
      " 15372 15419 15531 15588 15737 15798 15805 15815 15817 15839 15873 16106\n",
      " 16153 16187 16271 16380 16528 16627 16634 16640 16901 17021 17094 17122\n",
      " 17192 17214 17282 17391 17452 17545 17623 17798 17817 17822 17862 18019\n",
      " 18022 18032 18070 18146 18186 18195 18294 18315 18358 18447 18466 18475\n",
      " 18476 18488 18678 18767 18889 18898 18969 19065 19367 19437 19442 19750\n",
      " 19834 19871 19882 19918 19969 20019 20032 20047 20164 20203 20432 20456\n",
      " 20510 20545 20573 20585 20638 20682 20759 20769 20770 20848 20857 20859\n",
      " 20876 20940 21168 21195 21305 21348 21382 21387 21511 21654 21754 21763\n",
      " 21767 21771 21847 21877 21907 21932 21959 21969 21970 21992 22069 22074\n",
      " 22250 22314 22328 22330 22358 22472 22582 22704 22754 22853 22906 22913\n",
      " 22992 23022 23053 23251 23378 23444 23527 23590 23616 23916 23944 23994\n",
      " 24021 24038 24080 24085 24128 24259 24362 24523 24642 24677 24683 24737\n",
      " 24804 24843 24869 24895 24949 24950 25005 25116 25195 25200 25406 25435\n",
      " 25493 25498 25542 25586 25688 25707 25750 25770 25844 25988 26007 26023\n",
      " 26040 26045 26157 26201 26222 26306 26373 26383 26384 26390 26458 26460\n",
      " 26461 26489 26515 26582 26720 26786 26818 26889 27004 27038 27101 27217\n",
      " 27232 27276 27333 27368 27419 27484 27520 27521 27602 27632 27636 27645\n",
      " 27676 27729 27740 27802 27819 27899 28100 28179 28201 28283 28336 28486\n",
      " 28504 28543 28575 28796 28878 28893 28920 28926 28958 29066 29126 29142\n",
      " 29150 29175 29264 29303 29335 29336 29372 29380 29404 29429 29475 29494\n",
      " 29545 29584 29600 29625 29716 29752 29756 29868 29869]\n",
      "Selected points: [   49    77   173   273   308   382   387   435   440   442   576   638\n",
      "   654   787   796   822   875  1024  1057  1100  1135  1373  1508  1528\n",
      "  1687  1845  1871  2082  2248  2252  2371  2694  2717  2842  2934  2939\n",
      "  3201  3287  3295  3351  3438  3653  3755  3922  3937  4025  4239  4333\n",
      "  4349  4388  4458  4490  4687  4716  4908  4979  5069  5305  5499  5607\n",
      "  5642  5649  5653  5906  5923  5943  5983  6078  6373  6423  6543  6735\n",
      "  6812  6917  7114  7154  7321  7536  7599  7633  7901  7970  8035  8052\n",
      "  8344  8359  8380  8438  8465  8497  8617  8619  8640  8783  8795  8808\n",
      "  8919  8959  8967  9010  9206  9374  9449  9497  9528  9540  9711  9877\n",
      "  9993 10073 10081 10089 10162 10241 10255 10304 10601 10661 10799 10811\n",
      " 10837 10875 11140 11600 11664 11709 11711 11857 11959 11999 12249 12266\n",
      " 12319 12355 12597 12728 12778 12831 12897 12962 12995 13005 13008 13038\n",
      " 13070 13143 13197 13265 13346 13349 13496 13595 13597 13679 13791 13845\n",
      " 13859 13960 14080 14178 14203 14309 14329 14439 14451 14522 14779 14903\n",
      " 15036 15050 15051 15167 15281 15306 15420 15551 15588 15592 15643 15804\n",
      " 15815 15846 15849 15876 15909 15996 16069 16205 16220 16328 16411 16445\n",
      " 16621 16654 16709 16757 16774 16929 17062 17155 17211 17279 17293 17663\n",
      " 17753 17815 18040 18145 18291 18316 18678 18792 18902 19023 19081 19165\n",
      " 19286 19548 19571 19612 19687 19748 19763 19792 19895 19976 19981 20121\n",
      " 20195 20520 20553 20608 20676 20708 20720 20793 20834 20843 20952 20969\n",
      " 21018 21034 21041 21057 21119 21157 21185 21284 21351 21452 21770 21838\n",
      " 21854 21862 21889 21988 22011 22161 22231 22311 22406 22426 22778 22797\n",
      " 22891 22916 22960 22971 22984 23104 23158 23180 23219 23400 23411 23434\n",
      " 23482 23532 23541 23575 23576 23634 23791 23795 23881 23997 24008 24010\n",
      " 24021 24034 24166 24523 24602 24631 24686 24737 24789 24793 24888 24941\n",
      " 25024 25026 25054 25061 25107 25108 25164 25312 25318 25327 25387 25739\n",
      " 25824 25866 25895 25902 25966 26090 26152 26413 26553 26725 26776 27046\n",
      " 27170 27219 27254 27298 27307 27427 27527 27636 27663 27905 28009 28012\n",
      " 28146 28186 28223 28330 28448 28517 28536 28566 28677 28721 28747 28868\n",
      " 28964 28993 29069 29100 29106 29188 29300 29308 29383 29455 29531 29550\n",
      " 29586 29642 29678 29688 29729 29881 29900]\n",
      "9/9: lam=10 m=367 m_expected=571 probs_dist=(0.001907, 0.004723, 0.0004868)     \n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF\n",
    "X_test = np.random.randn(30000, 10)\n",
    "r = np.random.RandomState(42)\n",
    "D_test = bless(X_test, RBF(length_scale=10), 10, 10, r, 10, force_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((374,), (374, 10), (374,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_test.idx.shape, D_test.X.shape, D_test.probs.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0, 72], dtype=int64),)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF\n",
    "similarity_func = RBF(length_scale=10)\n",
    "# Create a dataset X\n",
    "# create a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 20)\n",
    "n, d = X.shape\n",
    "diag_norm = np.asarray(similarity_func.diag(X)) # it gives the diagonal of the kernel matrix = 1\n",
    "qbar=2\n",
    "ucb_init = qbar * diag_norm / n\n",
    "rng = np.random.RandomState(42)\n",
    "selected_init = rng.rand(n) <= ucb_init\n",
    "# force at least one sample to be selected\n",
    "selected_init[0] = 1\n",
    "D = CentersDictionary(idx=selected_init.nonzero(),\n",
    "                X=X[selected_init, :],\n",
    "                probs=np.ones(np.sum(selected_init)) * ucb_init[selected_init],\n",
    "                lam=n,\n",
    "                qbar=qbar)\n",
    "lam_final = 10;H =10\n",
    "lam_sequence = list(np.geomspace(lam_final, n, H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cupy not found, defaulting to numpy\n",
      "Tau estimates:\n",
      " [0.01781756 0.13935384 0.14266185 0.11074688 0.11260559 0.0886903\n",
      " 0.22677102 0.16531355 0.12992787 0.17682328 0.13984242 0.12395724\n",
      " 0.13961938 0.13813203 0.14040698 0.15188574 0.18981059 0.16841886\n",
      " 0.14274827 0.11090436 0.14115638 0.10050757 0.14082007 0.17502418\n",
      " 0.16917262 0.12885603 0.1346672  0.17618711 0.10901972 0.10625629\n",
      " 0.11069173 0.13687716 0.12374111 0.17275162 0.09592449 0.10571218\n",
      " 0.11415503 0.11081151 0.11795866 0.14704404 0.0849966  0.12746826\n",
      " 0.13138316 0.15315141 0.14677184 0.13325706 0.14712573 0.09551545\n",
      " 0.15996744 0.16137679 0.09633379 0.1022168  0.14941639 0.12413216\n",
      " 0.13176019 0.12759969 0.18369668 0.14822926 0.08636498 0.09541002\n",
      " 0.15641286 0.08660714 0.11272029 0.0932371  0.10120348 0.136261\n",
      " 0.1151679  0.14085564 0.09996032 0.12763398 0.11688655 0.13793836\n",
      " 0.01781756 0.11703629 0.17414882 0.11799593 0.17266841 0.14033381\n",
      " 0.17159034 0.13366023 0.14340924 0.15101238 0.15433955 0.12965325\n",
      " 0.13435802 0.11028859 0.15821955 0.10042003 0.08970963 0.08418811\n",
      " 0.0980339  0.17529156 0.07431837 0.18853986 0.09701734 0.14329687\n",
      " 0.11995118 0.09984947 0.10911615 0.14997715]\n"
     ]
    }
   ],
   "source": [
    "# Compute the estimates of all RLS using the compute_tau function\n",
    "lam_new = 0.2\n",
    "force_cpu = False\n",
    "tau_estimates = compute_tau(D, X, similarity_func, lam_new, force_cpu)\n",
    "print(\"Tau estimates:\\n\", tau_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "def leverage_scores_sampling(A, k):\n",
    "    # compute the SVD of A\n",
    "    U, s, Vt = svd(A, full_matrices=False)\n",
    "    # compute the leverage scores\n",
    "    leverage_scores = np.sum(U**2, axis=0)\n",
    "    # normalize the scores\n",
    "    p = leverage_scores / np.sum(leverage_scores)\n",
    "    # sample k columns with replacement according to p\n",
    "    indices = np.random.choice(A.shape[1], size=k, replace=False, p=p)\n",
    "    # construct the sampled matrix\n",
    "    B = A[:, indices]\n",
    "    return B\n",
    "\n",
    "# Example usage\n",
    "A = np.random.rand(10, 10)\n",
    "k = 5\n",
    "B = leverage_scores_sampling(A, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60993489, 0.92288084, 0.93574355, 0.19494487, 0.97382232],\n",
       "       [0.94025987, 0.18510137, 0.55533147, 0.58196297, 0.75512147],\n",
       "       [0.72093542, 0.48480624, 0.94979285, 0.9043004 , 0.13503545],\n",
       "       [0.5684627 , 0.60500483, 0.95821358, 0.19115622, 0.03347619],\n",
       "       [0.22509391, 0.10791163, 0.1243677 , 0.06702738, 0.97213703],\n",
       "       [0.3843179 , 0.44333009, 0.44619953, 0.53157502, 0.39892993],\n",
       "       [0.3495867 , 0.0983109 , 0.6739625 , 0.47256958, 0.6333704 ],\n",
       "       [0.86184064, 0.47377077, 0.49502417, 0.60095499, 0.4892662 ],\n",
       "       [0.24085047, 0.68571423, 0.68495285, 0.12538915, 0.36963362],\n",
       "       [0.64057427, 0.28317572, 0.67757728, 0.44232227, 0.30384955]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nystrom_embeddings(X, centers_dict, similarity_func, force_cpu=False):\n",
    "    xp = __load_gpu_module(force_cpu)\n",
    "    K_XD = xp.asarray(similarity_func(X, centers_dict.X))\n",
    "    U_DD, S_DD, _ = np.linalg.svd(xp.asnumpy(similarity_func(centers_dict.X, centers_dict.X)))\n",
    "    U_DD, S_root_inv_DD = __stable_invert_root(U_DD, S_DD)\n",
    "    K_DD_inv_sqrt = xp.asarray(U_DD * S_root_inv_DD.T)\n",
    "    return K_XD.dot(K_DD_inv_sqrt)\n",
    "\n",
    "\n",
    "def get_nystrom_matrix_approx(X, centers_dict, similarity_func, force_cpu=False):\n",
    "    B = get_nystrom_embeddings(X, centers_dict, similarity_func, force_cpu)\n",
    "    return B.dot(B.T)\n",
    "\n",
    "\n",
    "def get_nystrom_PCA(X, centers_dict, similarity_func, k=-1, force_cpu=False):\n",
    "    B = get_nystrom_embeddings(X, centers_dict, similarity_func, force_cpu)\n",
    "    if k > B.shape[1]:\n",
    "        raise ValueError('requesting k={} principal components, but the centers dictionary can only'\n",
    "                         'approximate m={} components.'.format(k, B.shape[1]))\n",
    "    U, Sigma, _ = np.linalg.svd(B,\n",
    "                                full_matrices=False,\n",
    "                                compute_uv=True)\n",
    "    return np.dot(U, np.diag(Sigma))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://misovalko.github.io/publications/calandriello2017efficient.pdf\n",
    "- [Ridge Regression and Provable Deterministic Ridge Leverage Score Sampling](https://arxiv.org/pdf/1803.06010v1.pdf)\n",
    "- https://deepai.org/publication/ridge-regression-and-provable-deterministic-ridge-leverage-score-sampling\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequentially process the kernel matrix Kn so that exact RLS computed on a small matrix ($K_t$ with $t << n$) are used to create an $\\epsilon$-accurate dictionary, which is then used to estimate the RLS for bigger kernels, which are in turn used to update the dictionary and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
